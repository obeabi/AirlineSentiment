{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airline_CNN_BertTok_BertEmbedLayer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obeabi/AirlineSentiment/blob/main/Airline_CNN_BertTok_BertEmbedLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJMaZXt8yoE-"
      },
      "source": [
        "# Airline Sentiment Dataset\r\n",
        "## Written by Abiola Obembe\r\n",
        "### Date: 2020-12-31\r\n",
        "\r\n",
        "### Goal: Train a classifiier to predict customer sentiment from customer review text (using Bert tokenizer and embedding layer)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqeNHhjp25v_"
      },
      "source": [
        "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiwXn9d50qi6"
      },
      "source": [
        "## Step 1: Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woeaqOfFP10F",
        "outputId": "2a30f127-b0bb-4d30-9c0f-24941e73194b"
      },
      "source": [
        "# install BERT library\r\n",
        "!pip install bert-for-tf2\r\n",
        "!pip install sentencepiece\r\n",
        "\r\n",
        "print(\"Packages installed!\")"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
            "Packages installed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjlZhoV_P3Uj",
        "outputId": "7c83fbcc-a784-4be4-af1b-2ba7e42a93f9"
      },
      "source": [
        "# Install tensorflow\r\n",
        "try:\r\n",
        "  %tensorflow_version 2.x\r\n",
        "except Exception:\r\n",
        "  pass\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import random\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow.keras import layers\r\n",
        "import bert\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "from bs4 import  BeautifulSoup\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\r\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\r\n",
        "plt.rcParams['image.cmap'] = 'gray'\r\n",
        "\r\n",
        "\r\n",
        "print(\"installation complete!\")\r\n",
        "print(tf.__version__)\r\n"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installation complete!\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Ru8XNKar0pGE",
        "outputId": "6735168d-783b-471e-8e82-f14cd32c2945"
      },
      "source": [
        "# Import dataset\r\n",
        "dataset = pd.read_csv('Tweets.csv', encoding= 'latin1', engine='python', quoting = 1)\r\n",
        "\r\n",
        "dataset.head()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "ZXcBWVgA2_co",
        "outputId": "ec8cc246-6d54-4c8c-b79d-8626cf738b89"
      },
      "source": [
        "# Drop columns not required\r\n",
        "dataset.drop(columns = ['tweet_id', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence',\r\n",
        "                        'airline', 'airline_sentiment_gold','name','negativereason_gold','retweet_count','tweet_coord',\r\n",
        "                        'tweet_created','tweet_location','user_timezone'], axis = 1, inplace = True)\r\n",
        "dataset.head(10)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>positive</td>\n",
              "      <td>@virginamerica Well, I didn'tâ¦but NOW I DO! :-D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment                                               text\n",
              "0           neutral                @VirginAmerica What @dhepburn said.\n",
              "1          positive  @VirginAmerica plus you've added commercials t...\n",
              "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
              "3          negative  @VirginAmerica it's really aggressive to blast...\n",
              "4          negative  @VirginAmerica and it's a really big bad thing...\n",
              "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
              "6          positive  @VirginAmerica yes, nearly every time I fly VX...\n",
              "7           neutral  @VirginAmerica Really missed a prime opportuni...\n",
              "8          positive  @virginamerica Well, I didn'tâ¦but NOW I DO! :-D\n",
              "9          positive  @VirginAmerica it was amazing, and arrived an ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghh1CI-p4MSM",
        "outputId": "0d113dcd-7f03-487d-f3c7-659c84383bc9"
      },
      "source": [
        "#  Investigate the number of distinct sentiments\r\n",
        "dataset['airline_sentiment'].value_counts()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    9178\n",
              "neutral     3099\n",
              "positive    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "G9xjlClf4Y8Y",
        "outputId": "0eabaa1c-0bfd-4064-937b-7a5f3d483b5b"
      },
      "source": [
        "# Lets visualize the sentiments\r\n",
        "count_classes = pd.value_counts(dataset['airline_sentiment'], sort = True)\r\n",
        "count_classes.plot(kind = 'barh', rot = 0)\r\n",
        "plt.title(\"Customer Sentiment Distribution\")\r\n",
        "plt.xticks(range(3))\r\n",
        "plt.xlabel(\"Sentiment\")\r\n",
        "plt.ylabel('Frequency')\r\n",
        "plt.show()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjZdgHY45JHr",
        "outputId": "4eb8758a-f2b2-4500-c338-3af90febdb24"
      },
      "source": [
        "# Let us check for missing values in both columns\r\n",
        "print(dataset.isnull().sum())\r\n",
        "\r\n",
        "missing_values = dataset.isnull().sum().sum()\r\n",
        "print('The total number of missing values in the dataframe is' , str(missing_values))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "airline_sentiment    0\n",
            "text                 0\n",
            "dtype: int64\n",
            "The total number of missing values in the dataframe is 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRkYlsNM6nzN"
      },
      "source": [
        "## Data Cleaning\r\n",
        "def clean_tweet(tweet):\r\n",
        "\r\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\r\n",
        "    # Removing the @\r\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\r\n",
        "    # Removing the URL links\r\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\r\n",
        "    # Keeping only letters\r\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\r\n",
        "    # Removing additional whitespaces\r\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\r\n",
        "    return tweet\r\n",
        "  "
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-xKQI1JTsZs"
      },
      "source": [
        "# Let's examine the corpus\r\n",
        "data_clean = [clean_tweet(tweet) for tweet in dataset.text]"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YitbsrA5TaO",
        "outputId": "ff3b5b2a-ab88-43b3-8af5-db5b1d0978ae"
      },
      "source": [
        "# Let us encode the labels into integers \r\n",
        "from sklearn import preprocessing \r\n",
        "  \r\n",
        "# label_encoder \r\n",
        "label_encoder = preprocessing.LabelEncoder() \r\n",
        "  \r\n",
        "# Encode labels in column 'species'. \r\n",
        "dataset['airline_sentiment']= label_encoder.fit_transform(dataset['airline_sentiment']) \r\n",
        "dataset['text']= data_clean\r\n",
        "dataset['airline_sentiment'].unique() "
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "6BwCr53qwTGP",
        "outputId": "88d99e83-7df1-413a-e443-9cfa73e0a385"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>What said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it's really aggressive to blast obnoxious ent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   airline_sentiment                                               text\n",
              "0                  1                                         What said.\n",
              "1                  2   plus you've added commercials to the experien...\n",
              "2                  1   I didn't today... Must mean I need to take an...\n",
              "3                  0   it's really aggressive to blast obnoxious ent...\n",
              "4                  0           and it's a really big bad thing about it"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP0OSKBoCvpd"
      },
      "source": [
        "## STEP 2: Tokenization with BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64a3R48BCzys"
      },
      "source": [
        "# Tokenization: BERT\r\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "                            trainable=False)\r\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\r\n"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY6Ed8-hpiB2"
      },
      "source": [
        "# Create function to tokenize text\r\n",
        "def encode_sentence(sent):\r\n",
        "  return [\"[CLS]\"] + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) + [\"[SEP]\"]"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyMO1g0JqDlF"
      },
      "source": [
        "# create data-input and data labels\r\n",
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean ]\r\n",
        "data_labels = dataset.airline_sentiment.values"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LANNz6mIlcbt",
        "outputId": "c5cd54fe-703c-465c-b9d1-39cd40f17887"
      },
      "source": [
        "# Let's examine the tokenizer in action\r\n",
        "encode_sentence(\"Dogs loves bones but hates oranges.\")"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 6077, 7459, 5944, 2021, 16424, 4589, 2015, 1012, '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkARl9Up0XPm"
      },
      "source": [
        "### Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzxTqf5vB2RU"
      },
      "source": [
        "def get_ids(tokens):\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "def get_mask(tokens):\r\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\r\n",
        "\r\n",
        "def get_segments(tokens):\r\n",
        "    seg_ids = []\r\n",
        "    current_seg_id = 0\r\n",
        "    for tok in tokens:\r\n",
        "        seg_ids.append(current_seg_id)\r\n",
        "        if tok == \"[SEP]\":\r\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\r\n",
        "    return seg_ids"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BB1FKQh8ZRjD",
        "outputId": "5bd136d2-3cc2-43c6-c68e-6383cf556aef"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\r\n",
        "                 for i, sent in enumerate(data_inputs)]\r\n",
        "random.shuffle(data_with_len)\r\n",
        "data_with_len.sort(key=lambda x: x[2])\r\n",
        "sorted_all = [([get_ids(sent_lab[0]),\r\n",
        "                get_mask(sent_lab[0]),\r\n",
        "                get_segments(sent_lab[0])],\r\n",
        "               sent_lab[1])\r\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-223-5c50e277a8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                 get_segments(sent_lab[0])],\n\u001b[1;32m      8\u001b[0m                sent_lab[1])\n\u001b[0;32m----> 9\u001b[0;31m               for sent_lab in data_with_len if sent_lab[2] > 7]\n\u001b[0m",
            "\u001b[0;32m<ipython-input-223-5c50e277a8c1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 get_segments(sent_lab[0])],\n\u001b[1;32m      8\u001b[0m                sent_lab[1])\n\u001b[0;32m----> 9\u001b[0;31m               for sent_lab in data_with_len if sent_lab[2] > 7]\n\u001b[0m",
            "\u001b[0;32m<ipython-input-222-3d487f057143>\u001b[0m in \u001b[0;36mget_ids\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_by_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36mconvert_by_vocab\u001b[0;34m(vocab, items)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3737"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OULnQO7B5ySm"
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\r\n",
        "                                             output_types = (tf.int32, tf.int32))\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes= ((None,), ()   ))"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "awOU9HJwfGMU",
        "outputId": "0bbff310-d104-42f0-ae9a-e5df7a66c291"
      },
      "source": [
        "# Inspect dataset\r\n",
        "next(iter(all_batched))"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]']].\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\n    dtype=dtype.as_numpy_dtype))\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nValueError: invalid literal for int() with base 10: '[CLS]'\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 921, in generator_py_func\n    sys.exc_info()[2])\n\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 702, in reraise\n    raise value.with_traceback(tb)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\n    dtype=dtype.as_numpy_dtype))\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]']].\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-225-adff5d4e9b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inspect dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]']].\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\n    dtype=dtype.as_numpy_dtype))\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nValueError: invalid literal for int() with base 10: '[CLS]'\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 921, in generator_py_func\n    sys.exc_info()[2])\n\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 702, in reraise\n    raise value.with_traceback(tb)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\n    dtype=dtype.as_numpy_dtype))\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]'], ['[CLS]', 2142, 28968, '[SEP]']].\n\n\n\t [[{{node PyFunc}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Dvw1dXL84Z"
      },
      "source": [
        "# split dataset into train and test set\r\n",
        "import math\r\n",
        "NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE)\r\n",
        "NB_BATCHES_TEST = NB_BATCHES// 10\r\n",
        "all_batched.shuffle(NB_BATCHES)\r\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\r\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "hxX75Gb-9c64",
        "outputId": "cda6a9d0-2353-4c3a-af9d-729232d3ad65"
      },
      "source": [
        "my_sent = [\"[CLS]\"] + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"Roses are red.\")) + [\"[SEP]\"]\r\n",
        "bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent),tf.int32),0),\r\n",
        "            tf.expand_dims(tf.cast(get_mask(my_sent),tf.int32),0),\r\n",
        "            tf.expand_dims(tf.cast(get_segments(my_sent),tf.int32),0)])"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-227-c7eab61dfca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Roses are red.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent),tf.int32),0),\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             tf.expand_dims(tf.cast(get_segments(my_sent),tf.int32),0)])\n",
            "\u001b[0;32m<ipython-input-222-3d487f057143>\u001b[0m in \u001b[0;36mget_ids\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_by_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36mconvert_by_vocab\u001b[0;34m(vocab, items)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 10529"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MeKHgAnNAbe"
      },
      "source": [
        "# Step 4: Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3E2QXV-NM0G"
      },
      "source": [
        "class DCNN(tf.keras.Model):\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 nb_filters=50,\r\n",
        "                 FFN_units=512,\r\n",
        "                 nb_classes=2,\r\n",
        "                 dropout_rate=0.1,\r\n",
        "                 training=False,\r\n",
        "                 name=\"dcnn\"):\r\n",
        "        super(DCNN, self).__init__(name=name)\r\n",
        "        \r\n",
        "        self.bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3', trainable= False)\r\n",
        "\r\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\r\n",
        "        \r\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,kernel_size=3, padding=\"valid\", activation=\"relu\")\r\n",
        "        \r\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\r\n",
        "        \r\n",
        "        self.pool = layers.GlobalMaxPool1D() # no training variable so we can\r\n",
        "                                             # use the same layer for each\r\n",
        "                                             # pooling step\r\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "\r\n",
        "        if nb_classes == 2:\r\n",
        "            self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\r\n",
        "        else:\r\n",
        "            self.last_dense = layers.Dense(units=nb_classes, activation=\"softmax\")\r\n",
        "\r\n",
        "\r\n",
        "    def embed_with_bert(self,all_token):\r\n",
        "       _,embs = self.bert_layer([all_tokens[:,0,:],\r\n",
        "                                 all_tokens[:,1,:],\r\n",
        "                                 all_tokens[:,2,:]])\r\n",
        "       return embs\r\n",
        "\r\n",
        "    def call(self, inputs, training):\r\n",
        "        x = self.embed_with_bert(inputs)\r\n",
        "        \r\n",
        "        x_1 = self.bigram(x)\r\n",
        "        x_1 = self.pool(x_1)\r\n",
        "        x_2 = self.trigram(x)\r\n",
        "        x_2 = self.pool(x_2)\r\n",
        "        x_3 = self.fourgram(x)\r\n",
        "        x_3 = self.pool(x_3)\r\n",
        "        \r\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\r\n",
        "        merged = self.dense_1(merged)\r\n",
        "        merged = self.dropout(merged, training)\r\n",
        "        output = self.last_dense(merged)\r\n",
        "        \r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoL7PvuQNbDZ"
      },
      "source": [
        "# Configuration details\r\n",
        "VOCAB_SIZE = len(tokenizer.vocab)\r\n",
        "\r\n",
        "EMB_DIM = 200\r\n",
        "NB_FILTERS = 100\r\n",
        "FFN_UNITS = 256\r\n",
        "NB_CLASSES = 3\r\n",
        "\r\n",
        "DROPOUT_RATE = 0.25\r\n",
        "\r\n",
        "NB_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXiP6189NtEg"
      },
      "source": [
        "# Let's DEFINE THE LOSS for the model\r\n",
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\r\n",
        "            emb_dim=EMB_DIM,\r\n",
        "            nb_filters=NB_FILTERS,\r\n",
        "            FFN_units=FFN_UNITS,\r\n",
        "            nb_classes=NB_CLASSES,\r\n",
        "            dropout_rate=DROPOUT_RATE)\r\n",
        "\r\n",
        "if NB_CLASSES == 2:\r\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\r\n",
        "                 optimizer=\"adam\",\r\n",
        "                 metrics=[\"accuracy\"])\r\n",
        "else:\r\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\r\n",
        "                 optimizer=\"adam\",\r\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkckNQU4mSWU"
      },
      "source": [
        "# Create a checkpoint\r\n",
        "checkpoint_path = \"./drive/My Drive/projects/Bert_for_NLP/ckpt/\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n",
        "\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "    print(\"Latest checkpoint restored!!\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZxBHbGNm_hI"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\r\n",
        "\r\n",
        "   def on_epoch_end(self, epoch, logs = None):\r\n",
        "     ckpt_manager.save()\r\n",
        "     print(\"Checkpoint saved at {}. \".format(checkpoint_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHvuNVEcNwH-"
      },
      "source": [
        "# train model\r\n",
        "Dcnn.fit(train_dataset,epochs=NB_EPOCHS, callbacks= [MyCustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5svksYtNOBM4"
      },
      "source": [
        "# Evaluate model\r\n",
        "results = Dcnn.evaluate(test_dataset)\r\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DYl2Vqqq3N4"
      },
      "source": [
        "# Make predictions\r\n",
        "def get_prediction(sentence):\r\n",
        "  tokens = encode_sentence(sentence)\r\n",
        "  inputs = tf.expand_dims(tokens, 0)\r\n",
        "\r\n",
        "  output = Dcnn(inputs, training= False)\r\n",
        "  sentiment = output\r\n",
        "\r\n",
        "  for i in range(len(sentiment)):\r\n",
        "    if np.argmax(sentiment) == 0:\r\n",
        "      print(\"The sentiment is negative\")\r\n",
        "\r\n",
        "    elif np.argmax(sentiment) == 1:\r\n",
        "      print(\"The sentiment is neutral\")\r\n",
        "\r\n",
        "    else:\r\n",
        "        print('The sentiment is positive')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WELSHlkFtZ0T"
      },
      "source": [
        "x= get_prediction(\"The food was lovely.\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}